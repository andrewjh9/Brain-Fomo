#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=preprocess_tuev
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=06:00:00
#SBATCH --output=%x_%j.out

# -------------------------------------------------------------------------------------
# TAKES ROUGHLY 2 hours
# -------------------------------------------------------------------------------------

module purge
module load 2023

echo "--- Environment After Loading Modules ---"
echo "Loaded modules:"
module list
echo "----------------------------------------"
echo "CUDA_HOME is set to: $CUDA_HOME"
echo "Path to nvcc: $(which nvcc)"
echo "----------------------------------------"

export PATH="$HOME/miniconda3/bin:$PATH"
eval "$(conda shell.bash hook)"

conda activate labram23
echo "Activated conda environment: $CONDA_DEFAULT_ENV"
echo "Python version: $(python --version)"
echo "Python path: $(which python)"
echo "Pip path: $(which pip3)"

echo "--- Checking PyTorch GPU Access ---"
srun python -uc "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available? {torch.cuda.is_available()}'); print(f'CUDA version built with: {torch.version.cuda}'); print(f'Device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"
echo "---------------------------------"

cd $HOME
cd ../../                                           
echo "[$(date)] Using dir: $(pwd)"                     

FILEPATH="home/scur0546/pascal/dataset_maker/make_TUEV.py"

echo "[$(date)] Running file: $FILEPATH"
srun python "$FILEPATH"

# Access
# "scratch-shared/scur0546/data/TUEV/tuh_eeg_events/v2.0.1/edf/processed/train"
# "scratch-shared/scur0546/data/TUEV/tuh_eeg_events/v2.0.1/edf/processed/val"
# "scratch-shared/scur0546/data/TUEV/tuh_eeg_events/v2.0.1/edf/processed/test"
